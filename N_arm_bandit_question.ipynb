{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this assignment you will be solving N-arm bandit problem\n",
    "\n",
    "#### You will first apply random approach and then epsilon greedy approach.\n",
    "\n",
    "**Follow the instruction and code accordingly**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the below cell to import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"\n",
    "    This class initilize the number of slot machines\n",
    "    and define the function to perform the actions.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_arms=10):\n",
    "        ### Assign number of arms (slot machines) for constructor variable\n",
    "        self.num_arms =    \n",
    "        \n",
    "        # for each arm, save a random probability for success\n",
    "        self._probs =  \n",
    "        \n",
    "        ### assign indexes to actions\n",
    "        self.action_space = \n",
    "\n",
    "    \"\"\"\n",
    "    define function try_arm\n",
    "    attrs: arm_num: the slot machine to try on (int)\n",
    "    return  reward 0 if random probability is less than the arm probabilty of success else 1\n",
    "    \"\"\"\n",
    "    def try_arm(self, arm_num):\n",
    "        # either succeed or fail randomly based on the arm's probability\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Agent that choose random action on every trail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the class RandomAgent with takes in the action_space\n",
    "and try random slot machines using choose_action function\n",
    "\"\"\"\n",
    "class RandomAgent():\n",
    "    def __init__(self,action_space):\n",
    "        ### pass the action_space through constructor variable\n",
    "        self.action_space = \n",
    "\n",
    "    # Choose a random action\n",
    "    def choose_action(self):\n",
    "        \"\"\"Returns a random choice of the available actions\n",
    "        return a random slot machine (int)\n",
    "        \"\"\"\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below code to run trials using random approach\n",
    "- The agent will run for 1000 trials\n",
    "- In each trail it chooses action based on choose_action defined in RandomAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "env = Environment(20)\n",
    "agent = RandomAgent(env.action_space)\n",
    "\n",
    "random_cumulative_reward = []\n",
    "\n",
    "total_reward = 0\n",
    "for i in range(1000):\n",
    "    action = agent.choose_action()\n",
    "    total_reward += env.try_arm(action)\n",
    "    random_cumulative_reward.append(total_reward)\n",
    "    \n",
    "print(\"Total reward after following 1000 trails using random approach {}\".format(random_cumulative_reward[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon Greey Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running sum formula\n",
    "$Q_n(a)  = Q_{n-1}(a) + \\frac{1}{N_n(a)}(R_n -  Q_{n-1}(a))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the class EpsilonGreetyAgent() which follows epsilon-greedy approach to perfrom actions\n",
    "class parametrs:\n",
    "action_space : number of arms (array of indexes)\n",
    "epsilon: probability of being greedy\n",
    "action_value: action_values of each arm initilized to zero (array)\n",
    "observation_counts: number of times each arm tried so far (array)\n",
    "\"\"\"\n",
    "class EpsilonGreedyAgent():\n",
    "    def __init__(self,action_space,epsilon=0.05):\n",
    "        self.action_space = action_space\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        ### initilize action value as array whose size is equal to action_space and all values as zero\n",
    "        self.action_values = \n",
    "        \n",
    "        ### initialize observation counts whose size is equal to action_space and all values as zero\n",
    "        self.observation_counts = \n",
    "\n",
    "    def choose_action(self):\n",
    "        \"\"\"Returns a random action from available actions\n",
    "           if sample from np.random.uniform(0,1) less than epsilon\n",
    "           else return the arm number with maximum action_value\"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "    \"\"\"\n",
    "    define function learn to update the observation_counts and action value\n",
    "    parameters:\n",
    "      action: arm number (int)\n",
    "      reward: reward retun by environment (define by try_arm)\n",
    "    \"\"\"\n",
    "    def learn(self,action,reward):\n",
    "        ### Based on the action (arm number) update the corresponding observation space by one\n",
    "        self.observation_counts[action] = \n",
    "        ### retrive the the action_value of the current action\n",
    "        current_val = \n",
    "\n",
    "        ### update the action_value as per the action_value function using running sum (refer the course card for the formula)\n",
    "        self.action_values[action] = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below code for the agent to follow epsilon-greedy approach\n",
    "- The agent will run for 1000 trials\n",
    "- Based on the rewards return the action_values are updated using learn() method \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = EpsilonGreedyAgent(env.action_space,epsilon=0.07)\n",
    "\n",
    "eps_greedy_cumulative_reward = []\n",
    "total_reward = 0\n",
    "for i in range(1000):\n",
    "    action = agent.choose_action()\n",
    "    reward = env.try_arm(action)\n",
    "    agent.learn(action,reward)\n",
    "    total_reward += reward\n",
    "    eps_greedy_cumulative_reward.append(total_reward)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "plt.style.use('fivethirtyeight')\n",
    "y_random = random_cumulative_reward\n",
    "y_epislon = eps_greedy_cumulative_reward\n",
    "x = np.arange(0, 1000)\n",
    "ax.set_yticklabels([])\n",
    "\n",
    "ax.plot(x,y_random, label = 'Random')\n",
    "ax.plot(x,y_epislon, label = 'Epsilon Greedy')\n",
    "ax.legend()\n",
    "ax.title.set_text('Random vs Epsilon-Greedy')\n",
    "ax.set_xlabel('Trials')\n",
    "ax.set_ylabel('Reward')\n",
    "\n",
    "print(\"Total reward after following 1000 trails using epsilon-greedy approach {}\".format(eps_greedy_cumulative_reward[-1]))\n",
    "with open(\"output.txt\", 'w') as file:\n",
    "    file.write(str(eps_greedy_cumulative_reward[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
